{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1cca665",
   "metadata": {},
   "source": [
    "# Fulltext analysis\n",
    "\n",
    "This notebook is a work in progress\n",
    "\n",
    "### To run this notebook, you need to install the following packages:\n",
    "\n",
    "- ```paperetl``` (Python)\n",
    "- ```GROBID``` (Java, [installation](https://grobid.readthedocs.io/en/latest/Install-Grobid/))\n",
    "- ```pandas``` (Python)\n",
    "\n",
    "### The notebook is sectioned into the following parts:\n",
    "\n",
    "- [Parsing PDFs](#Parsing-PDFs)\n",
    "- [Handling duplicates](#Handling-duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing PDFs\n",
    "\n",
    "# To use paperetl, active GROBID instance is needed\n",
    "# To use GROBID, JVM should also be installed\n",
    "# !wget https://github.com/kermitt2/grobid/archive/0.7.2.zip\n",
    "# !unzip 0.7.2.zip\n",
    "# !cd grobid-0.7.2\n",
    "# !./gradlew run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d293199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f16686",
   "metadata": {},
   "source": [
    "## Parsing PDFs\n",
    "\n",
    "- [Back to top](#Fulltext-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e120de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Google Scholar PDFs\n",
    "# !mkdir ./data/google-scholar/papers/json\n",
    "# !python -m paperetl.file data/google-scholar/papers json://data/google-scholar/papers/json\n",
    "\n",
    "# Parsing Springer PDFs\n",
    "# !mkdir ./data/springer/papers/json\n",
    "# !python -m paperetl.file data/springer/papers json://data/springer/papers/json\n",
    "\n",
    "# Parsing arXiv PDFs\n",
    "# !mkdir ./data/arxiv/papers/json\n",
    "# !python -m paperetl.file data/arxiv/papers json://data/arxiv/papers/json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_papers = glob.glob(\"./data/google-scholar/papers/json/*.json\")\n",
    "s_papers = glob.glob(\"./data/springer/papers/json/*.json\")\n",
    "a_papers = glob.glob(\"./data/arxiv/papers/json/*.json\")\n",
    "\n",
    "print(\"Parsed {0}, {1}, {2} papers from Google Scholar (various publishers), Springer, and arXiv; respectively.\".format(len(gs_papers), len(s_papers), len(a_papers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a56960",
   "metadata": {},
   "source": [
    "## Handling duplicates\n",
    "\n",
    "- [Back to top](#Fulltext-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = []\n",
    "all_papers_map = {}\n",
    "sources = [\"google-scholar\", \"springer\", \"arxiv\"]\n",
    "\n",
    "for source in sources:\n",
    "    source_papers = glob.glob(\"./data/{0}/papers/json/*.json\".format(source))\n",
    "    for source_paper in source_papers:\n",
    "        with open(source_paper) as json_file:\n",
    "            paper = json.load(json_file)\n",
    "            location_pdf = \"-\".join(source_paper.split(\"-\")[:-1]).replace(\"/papers/json/\", \"/papers/\") + \".pdf\"\n",
    "            paper_dict = {\"source\": source, \"location_json\": source_paper, \"location_pdf\": location_pdf, \"details\": paper}\n",
    "            all_papers.append(paper_dict)\n",
    "            if paper[\"title\"] in all_papers_map.keys():\n",
    "                all_papers_map[paper[\"title\"]].append(paper_dict)\n",
    "            else:\n",
    "                all_papers_map[paper[\"title\"]] = [paper_dict]\n",
    "    print(\"Added papers from source:\", source)\n",
    "print(\"Total papers collected:\", len(all_papers))\n",
    "print(\"Total unique titles collected:\", len(all_papers_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c27596",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = []\n",
    "\n",
    "for title in all_papers_map.keys():\n",
    "    if len(all_papers_map[title]) > 1:\n",
    "        paper_sources = []\n",
    "        paper_versions = []\n",
    "        paper_pdfs = []\n",
    "        paper_jsons = []\n",
    "        for paper in all_papers_map[title]:\n",
    "            paper_sources.append(paper[\"source\"])\n",
    "            paper_versions.append(paper[\"details\"])\n",
    "            paper_pdfs.append(paper[\"location_pdf\"])\n",
    "            paper_jsons.append(paper[\"location_json\"])\n",
    "        duplicates.append({\"title\": all_papers_map[title][0][\"details\"][\"title\"], \"sources\": paper_sources, \"versions\": paper_versions, \"pdfs\": paper_pdfs, \"jsons\": paper_jsons})\n",
    "print(\"Separated {} duplicate papers for further analysis\".format(len(duplicates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates_log = []\n",
    "\n",
    "# for duplicate in duplicates:\n",
    "#     duplicates_log.append({\"title\": duplicate[\"title\"], \"sources\": duplicate[\"sources\"], \"pdfs\": duplicate[\"pdfs\"], \"jsons\": duplicate[\"jsons\"], \"comment\": \"\"})\n",
    "    \n",
    "# with open(\"./log/duplicates-log.json\", \"w\") as duplicates_log_file:\n",
    "#     json.dump(duplicates_log, duplicates_log_file, indent=2)\n",
    "\n",
    "# No need to run, as this block has run before, and running this will overwrite the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./log/duplicates-log-commented.json\", \"r\") as duplicates_log_file:\n",
    "#     duplicates_log_commented = json.load(duplicates_log_file)\n",
    "#     for duplicate in duplicates_log_commented:\n",
    "#         print(duplicate[\"title\"], \"-->\", duplicate[\"comment\"])\n",
    "\n",
    "# No need to run, as this block has run before, and running this will overwrite the log file\n",
    "\n",
    "# Additionally deleted the duplicate paper: A_Method_to_Assess_Climate_Change_Induce(1).pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_papers = glob.glob(\"./data/google-scholar/papers/json/*.json\")\n",
    "s_papers = glob.glob(\"./data/springer/papers/json/*.json\")\n",
    "a_papers = glob.glob(\"./data/arxiv/papers/json/*.json\")\n",
    "\n",
    "print(\"After handling duplicates, {0}, {1}, {2} papers from Google Scholar (various publishers), Springer, and arXiv are left; respectively.\".format(len(gs_papers), len(s_papers), len(a_papers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f17e6",
   "metadata": {},
   "source": [
    "## Formatting and relocating JSONs\n",
    "\n",
    "- [Back to top](#Fulltext-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee090e4",
   "metadata": {},
   "source": [
    "**Some comments on the quality of the metadata of the papers so far:**\n",
    "\n",
    "- 'id' values are not unique, must be updated with unique ones\n",
    "- 'source' values are not complete, must be updated with unique local filepath\n",
    "- 'published' & 'publication' values are not reliable, though must keep as not to lose valuable information\n",
    "- 'affiliation' values are totally included in the 'affiliations' values\n",
    "- 'reference' values are not reliable, there might have been a parsing issue\n",
    "- 'tags' values were useful for parsing only\n",
    "- 'authors', 'title', 'entry' values must be kept as they are\n",
    "- 'sections' values must be connected under the same section title for each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2475e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data/papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\"google-scholar\", \"springer\", \"arxiv\"]\n",
    "\n",
    "i = 1\n",
    "\n",
    "for source in sources:\n",
    "    source_dir = \"./data/{0}/papers/json/\".format(source)\n",
    "    source_papers = glob.glob(\"{0}*.json\".format(source_dir))\n",
    "    print(\"Accessing {1} papers in {0}\\n\".format(source_dir, len(source_papers)))\n",
    "    \n",
    "    for source_paper in source_papers:\n",
    "        print(\"---> Paper no. {0} <---\\n\".format(i))\n",
    "        print(\"Processing paper from {0}\\n\".format(source_paper))\n",
    "        \n",
    "        with open(source_paper) as source_json:\n",
    "            paper = json.load(source_json)\n",
    "            paper_clean = {}\n",
    "            paper_clean[\"uuid\"] = str(uuid.uuid4())\n",
    "            paper_clean[\"source\"] = \"./data/{0}/papers/{1}\".format(source, paper[\"source\"])\n",
    "            for col in [\"entry\", \"title\", \"authors\", \"affiliations\", \"published\", \"publication\"]:\n",
    "                paper_clean[col] = paper[col]\n",
    "            \n",
    "            paper_sections = {}\n",
    "            for section in paper[\"sections\"]:\n",
    "                if section[\"name\"] not in paper_sections.keys():\n",
    "                    paper_sections[section[\"name\"]] = section[\"text\"]\n",
    "                else:\n",
    "                    paper_sections[section[\"name\"]] = \" \".join([paper_sections[section[\"name\"]], section[\"text\"]])\n",
    "            paper_clean[\"sections\"] = paper_sections\n",
    "            \n",
    "            paper_fulltext = \"\"\n",
    "            for section in paper_sections.keys():\n",
    "                paper_fulltext = paper_fulltext + \" \" + str(section)\n",
    "                paper_fulltext = paper_fulltext + \" \" + paper_sections[section]\n",
    "            paper_fulltext = \" \".join(paper_fulltext.split()).strip()\n",
    "            paper_clean[\"fulltext\"] = paper_fulltext\n",
    "            paper_words = len(paper_fulltext.split())\n",
    "            paper_clean[\"words\"] = paper_words\n",
    "            \n",
    "            target_paper = \"./data/papers/{0}.json\".format(paper_clean[\"uuid\"])\n",
    "            with open(target_paper, \"w\") as target_json:\n",
    "                json.dump(paper_clean, target_json, indent=2)\n",
    "                print(\"Saving paper to {0}\\n\".format(target_paper))\n",
    "                i += 1\n",
    "                \n",
    "# Log saved to formatting-relocating-log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ef407c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data/papers-short\n",
    "!mkdir ./data/papers-medium\n",
    "!mkdir ./data/papers-long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9b78b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_path = glob.glob(\"./data/papers/*.json\")\n",
    "\n",
    "for paper_path in papers_path:\n",
    "    with open(paper_path) as json_file:\n",
    "        paper = json.load(json_file)\n",
    "        if paper[\"words\"] > 15000:\n",
    "            target_paper = paper_path.replace(\"/papers/\", \"/papers-long/\")\n",
    "        elif paper[\"words\"] < 1500:\n",
    "            target_paper = paper_path.replace(\"/papers/\", \"/papers-short/\")\n",
    "        else:\n",
    "            target_paper = paper_path.replace(\"/papers/\", \"/papers-medium/\")\n",
    "        with open(target_paper, \"w\") as target_json:\n",
    "            json.dump(paper, target_json, indent=2)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8bf4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_path = glob.glob(\"./data/papers-medium/*.json\")\n",
    "papers_chunks = [papers_path[x:x+34] for x in range(0, len(papers_path), 34)]\n",
    "\n",
    "for i in range(len(papers_chunks)):\n",
    "    group = \"./data/papers-medium/group{0}\".format(i)\n",
    "    !mkdir {group}\n",
    "    for paper_path in papers_chunks[i]:\n",
    "        target_paper = paper_path.replace(\"/papers-medium/\", \"/papers-medium/group{0}/\".format(i))\n",
    "        os.rename(paper_path, target_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60e4fa",
   "metadata": {},
   "source": [
    "## Analyzing Springer abstracts with pyResearchInsights\n",
    "\n",
    "- [Back to top](#Fulltext-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyResearchInsights.Cleaner import cleaner_main\n",
    "\n",
    "abstracts_log_name = \"./LOGS/log/abstracts.txt\"\n",
    "status_logger_name = \"test_run\"\n",
    "cleaner_main(abstracts_log_name, status_logger_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac96061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyResearchInsights.Analyzer import analyzer_main\n",
    "\n",
    "abstracts_log_name = \"./LOGS/log/abstracts_CLEANED.txt\"\n",
    "status_logger_name = \"test_run\"\n",
    "analyzer_main(abstracts_log_name, status_logger_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyResearchInsights.NLP_Engine import nlp_engine_main\n",
    "abstracts_log_name = \"./LOGS/log/abstracts_CLEANED.txt\"\n",
    "status_logger_name = \"test_run\"\n",
    "nlp_engine_main(abstracts_log_name, status_logger_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3aee7",
   "metadata": {},
   "source": [
    "## Extracting information from fulltexts\n",
    "\n",
    "- [Back to top](#Fulltext-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "053dce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gereltuya/Downloads/spbu/ai-for-climate-action/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  3.10.6\n",
      "re:  2.2.1\n",
      "torch:  2.0.1+cu117\n",
      "transformers:  4.29.2\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print('Python: ', platform.python_version())\n",
    "print('re: ', re.__version__)\n",
    "print('torch: ', torch.__version__)\n",
    "print('transformers: ', transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8668e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7ec975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0f6e4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text, tokens, num_tokens, chunk_size=3000, overlap=100):\n",
    "    chunks = []\n",
    "    for i in range(0, num_tokens, chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "af530518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['uuid', 'source', 'entry', 'title', 'authors', 'affiliations', 'published', 'publication', 'sections', 'fulltext', 'words'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "58bcfb0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "mkdir: cannot create directory ‘./data/papers-medium/group0/processed’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11185 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 34\n",
      "Started processing file ./data/papers-medium/group0/6b203eaf-3d8c-4db9-b750-c9d49e6a5f9e.json\n",
      "Started processing chunk #0\n",
      "Started processing chunk #1\n",
      "Started processing chunk #2\n",
      "Started processing chunk #3\n",
      "Saving processed file ./data/papers-medium/group0/processed/6b203eaf-3d8c-4db9-b750-c9d49e6a5f9e.json\n",
      "\n",
      "2 / 34\n",
      "Started processing file ./data/papers-medium/group0/12e4d142-fdb0-499f-9d7e-1bcc27bcb8f3.json\n",
      "Started processing chunk #0\n",
      "Started processing chunk #1\n",
      "Started processing chunk #2\n",
      "Saving processed file ./data/papers-medium/group0/processed/12e4d142-fdb0-499f-9d7e-1bcc27bcb8f3.json\n",
      "\n",
      "3 / 34\n",
      "Started processing file ./data/papers-medium/group0/376b3793-53c5-4d34-91d9-2c712aef4934.json\n",
      "Started processing chunk #0\n",
      "Started processing chunk #1\n",
      "Started processing chunk #2\n",
      "Started processing chunk #3\n",
      "Started processing chunk #4\n",
      "Saving processed file ./data/papers-medium/group0/processed/376b3793-53c5-4d34-91d9-2c712aef4934.json\n",
      "\n",
      "4 / 34\n",
      "Started processing file ./data/papers-medium/group0/d24ce279-c83d-415a-b57d-66afc7d16b3a.json\n",
      "Started processing chunk #0\n",
      "Started processing chunk #1\n",
      "Started processing chunk #2\n",
      "Started processing chunk #3\n",
      "Saving processed file ./data/papers-medium/group0/processed/d24ce279-c83d-415a-b57d-66afc7d16b3a.json\n",
      "\n",
      "5 / 34\n",
      "Started processing file ./data/papers-medium/group0/93c4fa05-467a-4a1c-a545-0070d69dc66d.json\n",
      "Started processing chunk #0\n",
      "Started processing chunk #1\n",
      "Started processing chunk #2\n",
      "Saving processed file ./data/papers-medium/group0/processed/93c4fa05-467a-4a1c-a545-0070d69dc66d.json\n",
      "\n",
      "6 / 34\n",
      "Started processing file ./data/papers-medium/group0/e7cf0ecb-7b65-4679-b945-1d9a2a923ce4.json\n",
      "Started processing chunk #0\n",
      "Started processing chunk #1\n",
      "Started processing chunk #2\n",
      "Started processing chunk #3\n",
      "Started processing chunk #4\n",
      "Saving processed file ./data/papers-medium/group0/processed/e7cf0ecb-7b65-4679-b945-1d9a2a923ce4.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./data/papers-medium/group0/processed\n",
    "papers_path = glob.glob(\"./data/papers-medium/group0/*.json\")\n",
    "total = len(papers_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "for j, paper_path in enumerate(papers_path[28:]):\n",
    "    with open(paper_path) as json_file:\n",
    "        print(j+1, \"/\", total)\n",
    "        print(\"Started processing file {0}\".format(paper_path))\n",
    "        paper = json.load(json_file)\n",
    "        paper_tokens = tokenizer.encode(paper[\"fulltext\"])\n",
    "        paper_num_tokens = len(paper_tokens)\n",
    "        paper[\"tokens\"] = paper_num_tokens\n",
    "        paper_chunks = get_chunks(paper[\"fulltext\"], paper_tokens, paper_num_tokens)\n",
    "        paper_extractions = []\n",
    "        for i in range(len(paper_chunks)):\n",
    "            print(\"Started processing chunk #{0}\".format(i))\n",
    "            paper_extraction = {}\n",
    "            paper_chunk = tokenizer.decode(paper_chunks[i])\n",
    "            paper_extraction[\"chunk\"] = paper_chunk\n",
    "            prompt = f\"\"\"If specific AI solutions and their applications to specific climate action tasks are mentioned in the text within three backticks, extract the details in a JSON format with these keys: 'climate-change-problem', 'climate-action-proposed', 'climate-action-type' ('Mitigation' or 'Adaptation', or other applicable types), 'climate-action-sector', 'climate-action-country', 'machine-learning-solution', 'deep-learning-solution', 'other-solution', and 'context'. If not mentioned, extract available information with the same JSON format. Leave 'Unknown' where applicable. Do not repeat yourself. Limit each 'context' to 50 words. Aggregate values with commas where ('climate-action-proposed', 'machine-learning-solution', 'deep-learning-solution') values are the same. Use sentence case for all texts. ```{paper_chunk}```\"\"\"\n",
    "            paper_chunk_extraction = get_completion(prompt)\n",
    "            paper_extraction[\"extraction\"] = paper_chunk_extraction\n",
    "            paper_extractions.append(paper_extraction)\n",
    "        paper[\"extractions\"] = paper_extractions\n",
    "        paper_clean = {key:paper[key] for key in ['uuid', 'source', 'entry', 'title', 'authors', 'affiliations', 'published', 'publication', 'words', 'tokens', 'extractions']}\n",
    "        target_paper = paper_path.replace(\"/group0/\", \"/group0/processed/\")\n",
    "        with open(target_paper, \"w\") as target_json:\n",
    "            print(\"Saving processed file {0}\\n\".format(target_paper))\n",
    "            json.dump(paper_clean, target_json, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
